<!DOCTYPE html>
<html lang="" xml:lang="" xmlns="http://www.w3.org/1999/xhtml">

<head>
    <meta charset="utf-8" />
    <meta content="width=device-width, initial-scale=1" name="viewport" />
    <title>
        Open-GMOT: Open-Vocabulary Generic Multiple Object Tracking with Motion-Appearance Cost (MAC) SORT
    </title>
    <meta content="GLAMR" property="og:title" />
    <meta content="A typical pipeline for multi-object tracking (MOT) is to use a detector for object localization, and following re-identification (re-ID) for object association. This pipeline is partially motivated by recent progress in both object detec- tion and re-ID, and partially motivated by biases in existing tracking datasets, where most objects tend to have distin- guishing appearance and re-ID models are sufficient for es- tablishing associations. In response to such bias, we would like to re-emphasize that methods for multi-object tracking should also work when object appearance is not sufficiently discriminative. To this end, we propose a large-scale dataset for multi-human tracking, where humans have similar appearance, diverse motion and extreme articulation. As the dataset contains mostly group dancing videos, we name it “DanceTrack”. We expect DanceTrack to provide a better platform to develop more MOT algorithms that rely less on visual discrimination and depend more on motion analysis. We benchmark several state-of-the-art trackers on our dataset and observe a significant performance drop on DanceTrack when compared against existing benchmarks." name="description" property="og:description" />
    <meta content="https://github.com/DanceTrack" property="og:url" />
    <meta name="keywords" content="Multi-Object Tracking in Uniform Appearance and Diverse Motion">

    <link rel="stylesheet" href="css/style.css">
    <link rel="stylesheet" href="css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <script defer src="js/fontawesome.all.min.js"></script>
</head>

<body>
    <div class="navbar">
        <a href="#section1">Section 1</a>
        <a href="#section2">Section 2</a>
        <a href="#section3">Section 3</a>
    </div>

    <div class="n-title">
        <h1>
            Open-GMOT: Open-Vocabulary Generic Multiple Object Tracking with Motion-Appearance Cost (MAC) SORT
        </h1>
    </div>
    <div class="n-byline">
        <div class="byline">
            <ul class="authors">
                <li>
                    Anonymous
                </li>
            </ul>
            <ul class="authors affiliations">
                <li>
                    Anonymous
                </li>
            </ul>
            <!-- <ul class="authors affiliations">
                <li>
                    <sup>
                        *
                    </sup>
                    equal contribution
                </li> 
            </ul> -->
<!--             <ul class="authors venue">
                <li>
                    : 
                </li>
            </ul> -->
            <ul class="authors links">
                <li>
                    <a href="" target="_blank">
                        <button class="btn"><i class="fa fa-file-pdf"></i> Paper</button>
                    </a>
                </li>
                <li>
                    <a href="" target="_blank">
                        <button class="btn"><i class="fab fa-github"></i> Code</button>
                    </a>
                </li>
                <li>
                    <a href="" target="_blank">
                        <button class="btn"><i class="fab fa-github"></i> Dataset</button>
                    </a>
                </li> 
                <li>
                    <a href="" target="_blank">
                        <button class="btn"><i class="fab fa-youtube fa-w-18"></i> Demo</button>
                    </a>
                </li>
            </ul>
        </div>
    </div>

<!-- <iframe width="760" height="381" src="https://www.youtube.com/embed/IvxeJRg4rYg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
    
    <div class="n-article">
        <div class="n-page video">
<!--             <video class="centered shadow" width="100%" autoplay muted loop playsinline>
                <!-- t=0.001 is a hack to make iPhone show video thumbnail -->
<!--                 <source src="https://www.youtube.com/embed/IvxeJRg4rYg" type="video/mp4" /> -->
<!--             </video>  -->
<!--         <h2>
            Narrated Results Video
        </h2> --> 
        <div class="videoWrapper shadow">
            <iframe width="705" height="397" border-style=none src="https://www.youtube.com/embed/zuoxUaRC3l8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>            
            
            <div class="videocaption" style="margin-bottom: 1rem">
                <div>
                    Demo of our Tracking framework for Generic Object scenario.
                </div>   
            </div>
        </div>

        <h2 id="abstract">
            Abstract
        </h2>
        <p>
            Despite significant progress in Multi-Object Tracking (MOT) approaches in recent years, they still suffer from limitations, including their heavy reliance on prior knowledge of tracking targets, which necessitates costly annotation of large labeled datasets. Consequently, MOT methods are limited to a small set of predefined categories and struggle with unseen objects. On the other hand, Generic Multiple Object Tracking (GMOT), which aims to track objects of a particular generic type, requires less prior information about the targets. However, most existing GMOT approaches follow a one-shot paradigm, relying mainly on the initial bounding box, which hinders their ability to handle variants such as viewpoint, lighting, occlusion, and scale.

In this paper, we introduce a novel Open-Vocabulary GMOT, called Open-GMOT, which can track never-seen object categories with zero training examples. Our Open-GMOT is designed to track objects of a specific generic type category by introducing an include/exclude whole-part structure. To address false positives and false negatives, we propose a long-short spatio-temporal memory that stores high-confidence true positives of the tracked objects. Moreover, we address the challenge of tracking high-similarity appearance objects of the same genetic type category in GMOT by proposing an association method that integrates visual appearance with motion-based matching. Comprehensive experiments and extensive ablation studies on GMOT-40 dataset, AnimalTrack, and DanceTrack show that the proposed Open-GMOT outperforms prior state-of-the-art (SOTA) GMOT methods. Source code is available upon acceptance.
        </p>
        
        <h2>
            Method
        </h2>
        <div>
            <img class="figure" src="media/open-gmot.jpg" width="100%" alt="Pipeline Overview">
        </div>
        <p>
            An illustration of our proposed Open-CSOD for detecting objects using input prompt. It comprises two modules of IE-WP 
            strategy and LSM mechanism to eliminate FP from pre-trained VL models. Open-CSOD effectively reduces the reliance on
            fixed threshold and enhances the accuracy of the detection by introducing an adaptable threshold using Ztable lookup table.
        </p>
        <p>
        Our contributions:
            <ul>
              <li>Open-CSOD is introduced to identify objects with specific characteristics of a particular generic type without the need of training data.</li>
              <li>MAC-SORT is proposed to fuse visual representation and motion, facilitating the tracking of objects with highly similar appearances and intricate motion patterns.</li>
              <li>Open-GMOT, an open-vocabulary generic multiple object tracking framework, is introduced to tackle the challenges inherent in GMOT without the reliance on training data.</li>
              <li>Extensive experiments, comparisons, and ablation studies are conducted across multiple datasets, including GMOT-40, AnimalTrack, and DanceTrack, to validate the efficacy and adaptability of the proposed Open-GMOT framework.</li>  
            </ul>
        </p>
    
         <h2 id="results">
            Results
        </h2>
        
        <h3 class="results" id="oracle">
            Oracle Analysis
        </h3>
        <div>
            <img class="figure" src="media/oracle.jpg" width="100%" alt="Oracle Analysis">
        </div>
        <p>
        Oracle analysis of different association models on MOT17 and DanceTrack validation set, where the detection boxes are ground-truth boxes. The result shows the evident increased difficulty of performing multi-object tracking on DanceTrack than MOT17 dataset.
        </p>
        
        <h3 class="results" id="benchmark">
            Benchmark Result
        </h3>
        <div>
            <img class="figure" src="media/benchmark.jpg" width="100%" alt="Benchmark Result">
        </div>
        <p>
        Benchmark results of investigated algorithms on MOT17 and DanceTrack test set. DanceTrack makes detection easier (higher MOTA and DetA scoers) but still brings significant tracking performance drop compared to MOT17 (lower HOTA, AssA and IDF1 scores). This result reveals the bottleneck of multi-object tracking on DanceTrack is on the association part.
        </p>

        <h3 class="results" id="benchmark">
            Association Strategy
        </h3>
        <div>
            <img class="figure" src="media/motion.jpg" width="100%" alt="Benchmark Result">
        </div>
        <p>
        Comparisons of different association strategies on DanceTrack validation set. The detection results are output by the same YOLOX detector. Both Kalman filter and LSTM outperform naive IoU association by a large margin, indicating the great potential of motion models in tracking objects, especially when appearance cues are not reliable. We expect to see more researches in this field.
        </p>

        <h2 id="rebuttal-aaai">
            Rebuttal Section
        </h2>
        <div class="chat-container">
            <div class="recipient-message">
                <strong>Reviewer #1:</strong> Hello, how are you today?
            </div>
    
            <div class="sender-message">
                <strong>Answer:</strong> I'm doing well, thank you! How can I help you?
            </div>
            <br>
    
            <div class="recipient-message">
                <strong>Reviewer #2:</strong> I have a question about your product.
            </div>
    
            <div class="sender-message">
                <strong>Answer:</strong> Sure, I'd be happy to help. What's your question?
            </div>
            <br>
        </div>
        <p> </p>
        
    </div>

    <footer>
        <div class="footer-content">
            <p>&copy; 2023 Website for AAAI 2024 rebuttal</p>
        </div>
    </footer>
</body>

</html>
    
                   
