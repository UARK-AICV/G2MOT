<!DOCTYPE html>
<html lang="" xml:lang="" xmlns="http://www.w3.org/1999/xhtml">

<head>
    <meta charset="utf-8" />
    <meta content="width=device-width, initial-scale=1" name="viewport" />
    <title>
        Enhanced Kalman with Adaptive Appearance Motion SORT for Grounded Generic Multiple Object Tracking
    </title>
    <meta content="G2MOT" property="og:title" />
    <meta content="A typical pipeline for multi-object tracking (MOT) is to use a detector for object localization, and following re-identification (re-ID) for object association. This pipeline is partially motivated by recent progress in both object detec- tion and re-ID, and partially motivated by biases in existing tracking datasets, where most objects tend to have distin- guishing appearance and re-ID models are sufficient for es- tablishing associations. In response to such bias, we would like to re-emphasize that methods for multi-object tracking should also work when object appearance is not sufficiently discriminative. To this end, we propose a large-scale dataset for multi-human tracking, where humans have similar appearance, diverse motion and extreme articulation. As the dataset contains mostly group dancing videos, we name it ‚ÄúDanceTrack‚Äù. We expect DanceTrack to provide a better platform to develop more MOT algorithms that rely less on visual discrimination and depend more on motion analysis. We benchmark several state-of-the-art trackers on our dataset and observe a significant performance drop on DanceTrack when compared against existing benchmarks." name="description" property="og:description" />
    <meta content="https://github.com/DanceTrack" property="og:url" />
    <meta name="keywords" content="Generic Multi-Object Tracking in Uniform Appearance and Diverse Motion">

    <link rel="stylesheet" href="css/style.css">
    <link rel="stylesheet" href="css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <script defer src="js/fontawesome.all.min.js"></script>
    <script src="js/util.js"></script>
</head>

<body>
    <div class="navbar">
        <h3>G2MOT demo website</h3>
        <ul>
            <li><a href="index.html">Home</a></li>
            <li><a href="result.html">Result</a></li>
            <li><a href="code.html">Code</a></li>
            <li><a href="dataset.html">Dataset</a></li>
        </ul>
        <script>
            // Get the current URL
            var currentURL = window.location.href;
        
            // Select all navigation links
            var navLinks = document.querySelectorAll('.navbar a');
        
            // Loop through the links to find the active one
            for (var i = 0; i < navLinks.length; i++) {
                var linkURL = navLinks[i].href;
        
                // Check if the current URL contains the link's URL
                if (currentURL.indexOf(linkURL) !== -1) {
                    // Add the "active" class to the link
                    navLinks[i].classList.add('active');
                }
            }
        </script>
    </div>
    

    <div class="n-title">
        <h1>
            Enhanced Kalman with Adaptive Appearance Motion SORT for Grounded Generic Multiple Object Tracking
        </h1>
    </div>
    <div class="n-byline">
        <div class="byline">
            <ul class="authors">
                <li>
                    This work has been submitted to ECCV 2024
                </li>
            </ul>
            <p style="text-align: center; color: slategray">PDF version will be published soon</p>
            <!-- <ul class="authors affiliations">
                <li>
                    Anonymous
                </li>
            </ul> -->
            <ul class="authors links">
                <!-- <li>
                    <a href="" target="_blank">
                        <button disabled class="btn"><i class="fa fa-file-pdf"></i> Paper</button>
                    </a>
                </li>
                <li>
                    <a href="" target="_blank">
                        <button disabled class="btn"><i class="fab fa-github"></i> Code</button>
                    </a>
                </li>
                <li>
                    <a href="" target="_blank">
                        <button disabled class="btn"><i class="fab fa-github"></i> Dataset</button>
                    </a>
                </li>  -->
                <li>
                    <a href="https://www.youtube.com/playlist?list=PLSf1X3oNUW2se2U111HJ3wkSnZ7dcVGC_" target="_blank">
                        <button class="btn"><i class="fab fa-youtube fa-w-18"></i> Demo</button>
                    </a>
                </li>
            </ul>
        </div>
    </div>

    
    <div class="n-article">
        <div class="image-container">
            <div>
                <img class="figure zoomable" src="media/G2MOT-teaser.png" width="100%" alt="Sample overview">
                <p style="text-align: center; font-weight: bold;">Comparison between OneShot-GMOT (OS-GMOT) (left) and our Grounded-GMOT (right)</p>
            </div>
            <!-- Add more images with captions as needed -->
        </div>

        <h2 id="introduction">
            Introduction
        </h2>
        <p>
            In response to the limitations of traditional <strong>Multiple Object Tracking (MOT)</strong> systems, particularly their struggle with diverse and dynamic environments such as <em>surveillance and autonomous driving</em>, our research introduces groundbreaking advancements. Our motivation stems from the need for more adaptable, inclusive tracking technologies capable of handling the complexities of real-world scenarios. With this goal in mind, we present our contributions:<br><br>
        </p>

        <div>
            <img class="figure zoomable" src="media/different_input.png" width="100%" alt="Pipeline Overview">
        </div>

        <p>
<strong>Grounded-GMOT</strong>: This novel tracking paradigm leverages natural language processing, enabling the tracking of objects with varied attributes across different settings. This approach significantly expands the applicability of MOT by using descriptive language as a flexible, powerful tool for tracking.<br>
<strong>G<sup>2</sup>MOT dataset</strong>: We unveil a large-scale dataset that surpasses existing collections in both diversity and size. The G<sup>2</sup>MOT dataset is designed to support the nuanced needs of Grounded-GMOT, providing a rich resource for developing and testing advanced tracking systems.<br>
<strong>KAM-SORT</strong>: Our innovative tracking methodology combines camera motion analysis with adjustments in the balance between motion and appearance information. KAM-SORT represents a significant improvement in tracking accuracy and robustness, addressing the dynamic challenges of real-world object tracking.<br><br>
Through these contributions, we aim to push the boundaries of what's possible in object tracking, offering solutions that are not only technically advanced but also broadly accessible and adaptable to the evolving demands of various applications.<br>
        </p>
        
        <!------------------------------------------------------Section Boundary-------------------------------------------------->
        <h2 id="G2MOT">
            G<span class="superscript">2</span>MOT Dataset
        </h2>
        <p>Ensuring a fair assessment of GMOT methods demands a dataset of consistent quality, free from annotator bias, and with a clearly defined problem setup. To offer comprehensive coverage of real-world scenarios across different research domains, our released dataset embodies two characteristics:</p>
        <ol>
            <li><span class="emphasis">Diversity:</span> integrating diverse object categories from various sources, encompassing a broad spectrum of classes and diverse properties such as motion, occlusion, appearance similarity, and density. Additionally, it employs high-level semantics like <span class="code">player</span>, <span class="code">athlete</span>, <span class="code">referee</span>, etc., to describe objects in complex contexts, rather than using generic terms like <span class="code">person</span>.</li>
            <li><span class="emphasis">Fine-Grained Annotation:</span> alongside capturing detailed visual attributes like color, texture, and attachments, it offers extensive textual descriptions with existing synonyms alongside captions.</li>
        </ol>
        <div class="image-container">
            <div>
                <img class="figure zoomable" src="media/G2MOT-data_sample.png" width="100%" alt="Sample overview">
                <p style="text-align: center; font-weight: bold;">Examples to illustrate the efficacy of IE-Strategy. Left: Output from pre-trained VLM. Right: Output from IE-Strategy.</p>
            </div>
            <!-- Add more images with captions as needed -->
        </div>
        <p>
            <b><u><span style="font-size: 20px;">Module 1: Diversity</span></u></b> The dataset integrates a wide array of object categories from various sources. This diversity encompasses a broad spectrum of classes and properties, including motion, occlusion, appearance similarity, and density. Notably, it uses high-level semantics (like <code>player</code>, <code>athlete</code>, <code>referee</code>) to describe objects in complex contexts, moving beyond generic descriptors.
        </p>

        <div>
            <img class="figure zoomable" src="media/attribute_wordcloud.png" width="100%" alt="wordcloud">
            <p style="text-align: center; font-weight: bold;">Statistical information of our proposed G<sup>2</sup>MOT dataset.</p>
        </div>

        <div><br><br></div>
        <p>
            <b><u><span style="font-size: 20px;">Module 2: Fine-Grained Annotation</span></u></b> Beyond visual attributes (color, texture, attachments), the dataset provides extensive textual descriptions and synonyms. These annotations offer a granular view of each object, enhancing the dataset's utility for rigorous GMOT method assessment.
        </p>
        <div>
            <img class="figure zoomable" src="media/annotation_example.png" width="100%" alt="anno_exp">
            <p style="text-align: center; font-weight: bold;">Demonstration of superset and subset from <strong>horse_4</strong>and <strong>car-3</strong> in our proposed G<sup>2</sup>MOT dataset.</p>
            <br>
        </div>

        <div><br></div>

        <p style="font-size: 22px;">
            <b><span style=" font-weight: bold; font-family: 'Courier New', Courier, monospace;">KAM-SORT</span></b>:
        </p>
        <p>
            The KAM-SORT method addresses tracking within the Generic Multiple Object Tracking (GMOT) setting. It computes the cost matrix between existing tracks <span style="font-family: 'Times New Roman', Times, serif; background-color: #f8f8f8; padding: 5px; display: inline-block;">ùíØ</span> and new detections <span style="font-family: 'Times New Roman', Times, serif; background-color: #f8f8f8; padding: 5px; display: inline-block;">ùíü</span> using a novel approach that combines motion and visual appearance cues.
        </p>
        <p>
            Key contributions and advantages of this method include:
        </p>
        <ul>
            <li>
                <strong>Adaptive weighting of appearance and motion cues</strong> based on the homogeneity of object appearances, prioritizing motion over appearance when necessary.
            </li>
            <li>
                <strong>Use of an uncertainty revision parameter</strong> to better manage fast motion and object deformation, thus reducing mismatches in tracking.
            </li>
        </ul>
        <p>
            Overall, the KAM-SORT method exhibits a strategic balance between visual and motion cues, addressing the challenges presented by the high similarity of objects in GMOT. This advancement represents a significant step forward in tracking technology, especially in scenarios with rapid motion and deformation.
        </p>

        <div class="algorithm-wrapper">
            <div class="algorithm-box">
                <div class="algorithm-title">Algorithm 1: Kalman++ algorithm</div>
    
                <div class="algorithm-section">
                    <div class="algorithm-section-title">Data:</div>
                    <div><span class="algorithm-var">D, T</span>: set of detection boxes at current frame and tracks at the previous frame.</div>
                    <div><span class="algorithm-var">Œ±, s</span>: parameters of uncertainty revise and rescale factor.</div>
                </div>
    
                <div class="algorithm-section">
                    <div class="algorithm-section-title">Model:</div>
                    <div><span class="algorithm-var">C</span>: score matrix defined in Equation 5.</div>
                    <div><span class="algorithm-var">M</span>: bipartite matching function.</div>
                    <div><span class="algorithm-var">K<sub>p</sub>, K<sub>u</sub></span>: Kalman Filter predict and update.</div>
                    <div><span class="algorithm-var">BC, IoU</span>: function compute box center and IoU.</div>
                </div>
    
                <div class="algorithm-section">
                    <div class="algorithm-section-title">Output:</div>
                    <div><span class="algorithm-var">T'</span> set of new tracks.</div>
                </div>
    
                <div class="algorithm-section algorithm-steps">
                    <div class="algorithm-section-title">Algorithm Steps:</div>
                    <ol>
                        <li><span class="code">&#94;x, P = K<sub>p</sub>(T);</span> <span class="algorithm-comment">// Get estimated location and error covariance.</span></li>
                        <li><span class="code">S = C(&#94;x, D);</span> <span class="algorithm-comment">// Compute matching score between estimation and detection.</span></li>
                        <li><span class="code">DT<sub>m</sub>, D<sub>r</sub>, T<sub>r</sub> = M(S);</span> <span class="algorithm-comment">// 1st-round association produce matched pairs DT<sub>m</sub>, unmatched detections D<sub>r</sub>, and unmatched tracks T<sub>r</sub>.</span></li>
                        <li><span class="code">S<sub>IoU</sub> = IoU(D<sub>r</sub>, T<sub>r</sub>);</span> <span class="algorithm-comment">// 2nd-round associate unmatched ones.</span></li>
                        <li><span class="code">DT<sub>r</sub> = M(S<sub>IoU</sub>);</span> <span class="algorithm-comment">// Rematched pairs from remaining detections and tracks.</span></li>
                        <li>For <span class="code">(i<sub>d</sub>, i<sub>t</sub>)</span> in <span class="code">DT<sub>r</sub></span> do
                            <ul>
                                <li class="algorithm-comment">// i<sub>d</sub>: detection index, i<sub>t</sub>: track index.</li>
                                <li><span class="code">c<sup>min</sup> = &#94;x<sub>i<sub>t</sub></sub>[:2] - Œ± * sqrt(P[:2]);</span></li>
                                <li><span class="code">c<sup>max</sup> = &#94;x<sub>i<sub>t</sub></sub>[:2] + Œ± * sqrt(P[:2]);</span></li>
                                <li><span class="code">c = BC(D<sub>i<sub>d</sub></sub>);</span></li>
                                <li>If <span class="code">c > c<sup>min</sup> &amp;&amp; c < c<sup>max</sup></span> then
                                    <li><span class="code">DT<sub>m</sub> = DT<sub>m</sub> ‚à™ {(i<sub>d</sub>, i<sub>t</sub>)}</span>;</li>
                                </li>
                            </ul>
                        </li>
                        <li><span class="code">T' = K<sub>u</sub>(DT<sub>m</sub>);</span> <span class="algorithm-comment">// Update matched tracks.</span></li>
                    </ol>
                </div>
            </div>
        </div>
          
    </div>

    <footer>
        <div class="footer-content">
            <p style="text-align: center;">&copy; Website for ECCV2024's submission</p>
        </div>
    </footer>
</body>

</html>
    
                   
